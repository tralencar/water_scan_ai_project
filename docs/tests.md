# Automated Testing with Pytest

This project uses the **[Pytest](https://docs.pytest.org/en/stable/)** library to perform automated tests, ensuring code quality, reliability, and maintainability.

## ğŸ”¹ Test Structure

The tests are organized in the `tests/` directory, separated by component:

<pre>ğŸ“‚ WATER_SCAN_AI                           âœ… (Project root directory)</pre>
<pre>â”œâ”€â”€ ğŸ“‚ tests                               âœ… (Test folder)</pre>
<pre>â”‚    â”œâ”€â”€ conftest.py                       ğŸ“Œ Reusable fixtures (e.g., mock data)</pre>
<pre>â”‚    â”œâ”€â”€ test_data_pipeline.py             ğŸ“Œ Tests for the data pipeline</pre>
<pre>â”‚    â”œâ”€â”€ test_model_trainer.py             ğŸ“Œ Tests for training with RandomForest + Optuna</pre>
<pre>â”‚    â”œâ”€â”€ test_trainer_factory.py           ğŸ“Œ Tests for the trainer factory</pre>

## ğŸ”¹ Tools Used

* `pytest`: main testing framework
* `pytest-cov`: code coverage analysis
* `monkeypatch`: mocking of external functions and methods
* `tmp_path fixture`: creation of temporary test files

## ğŸ”¹ Fixtures (conftest.py)

The `conftest.py` file defines reusable data and configuration for multiple tests.

## ğŸ”¹ Tests Created

âœ… 1) `test_data_pipeline.py` <br>

* `test_pipeline_load_and_clean` <br>
ğŸ§ª Tests reading a CSV and handling missing values. <br>
ğŸ“ **Note:**
Simulates a `.csv` file with missing values and checks whether the `fillna(median)` method in `DataPipeline` correctly removes NaNs. Ensures the pipeline starts with clean data â€” essential to avoid model errors.

* `test_data_preprocessing_split_and_smote` <br>
ğŸ§ª Verifies train/test split and SMOTE application. <br>
ğŸ“ **Note:**
Checks that the data is split with stratification (maintaining the `Potability` proportion) and that SMOTE correctly balances the classes. This guarantees balanced training and prevents bias toward the majority class.

âœ… 2) `test_trainer_factory.py` <br>

* `test_create_trainer_success` <br>
ğŸ§ª Tests successful creation of a RandomForest trainer. <br>
ğŸ“ **Note:**
Verifies that the factory returns the correct instance (`RandomForestTrainer`) when receiving `random_forest` as the input. A key test for validating the modularity of the system through the Factory pattern.

* `test_create_trainer_invalid_model` <br>
âš ï¸ Tests error handling when providing an invalid model. <br>
ğŸ“ **Note:**
Ensures the system raises a `ValueError` when an unsupported model type is passed. Important for protecting the API from misuse and anticipating production failures.

âœ… 3) `test_model_trainer.py` <br>

* `test_optuna_fake` <br>
ğŸ§ª Tests Optuna optimization execution using monkeypatch, without running a real session. <br>
ğŸ“ **Note:**
Uses `monkeypatch` to replace the `objective()` method and force a fixed return (e.g., `0.9`). This enables testing the Optuna optimization flow without running the actual training or MLflow logging. Ideal for reducing execution time and safely simulating real environments.

## ğŸ”¹ Running the Tests

You can run the tests with:

```bash
make test
```

Or directly using Poetry:

`poetry run pytest --cov=src --cov-report=term-missing`

## ğŸ”¹ Code Coverage

Test coverage is automatically generated via `pytest-cov`. To view it:

`poetry run pytest --cov=src --cov-report=term-missing`

ğŸ’¡ Use --cov-fail-under=27 to require a minimum of 27% coverage.

## ğŸ”¹ ConfiguraÃ§Ã£o do Pytest

The project uses the `.pytest.ini` file to configure the correct path.

ğŸ“Œ Notes (Best practices followed): <br>
âœ… Modular test structure <br>
âœ… Use of fixtures to avoid repetition <br>
âœ… Component mocking with `monkeypatch` <br>
âœ… Independent and isolated tests <br>
âœ… Code coverage with `pytest-cov` <br>
